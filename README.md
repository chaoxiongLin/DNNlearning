# DNNlearning
## 华南理工大学研究生课程《深度学习》复习资料

### 1 卷积神经网络的层次结构及各部分的作用？
答：（1）卷积层：每一个卷积层包含多个特征映射，通过一种卷积滤波器提取的一种特征。（2）采样层：作用是基于局部相关性原理进行采样，从而减少数据量的同时保留有用信息。（3）连接层：每个神经元都被全连接到上一层的神经元，本质就是传统的神经网络，其目的是通过连接层和输出层的连接完成识别任务。
### 2 卷积神经网络的计算
答：神经网络的计算要看原始“图像”的大小和卷积核的大小，还要看stride的大小和padding的大小。假设原始大小为N * N，卷积核大小为m * m，stride=a，padding=b。那么通过这个卷积核后，得到的输出层的大小为(N+2b-m)/(a) * (N+2b-m)/(a)。
### 3 监督学习及有哪些具体算法
答：监督学习指的是模型所需的数据是由标签的，每个训练样本都是由数据特征和标签组成的，模型能够通过学习到特征和标签之间的映射关系。具体算法有：（1）线性回归：模型旨在找到输入特征和输出之间的线性关系。（2）逻辑回归：针对二分类问题。（3）决策树：用于回归和分类，这里的回归是得到一个阈值。（4）支持向量机。（5）KNN：通过计算新数据点与训练数据点之间的距离，找到K个邻居进行预测。（6）朴素贝叶斯。
### 4 无监督学习及有哪些具体算法
答：无监督学习是指那些能处理没有数据标签的数据，旨在挖掘数据中的模式、结构或分布，主要用来做聚类、降维和异常检测。（1）K-means聚类：将数据划分为k簇，每一个簇由一个质心代表，目标是最小化数据点到中心的距离。（2）层次聚类。（3）高斯混合模型：假设数据是由若干个高斯分布组成的，通过最大化算法（EM算法）估计参数。（4）主成分分析，降维技术。（5）t-SNE：一种非线性降维技术，通过保持高维数据点在低维空间中的临近关系进行降维。（6）自编码器，一种从自己到自己的神经网络。
### 5 BP 神经网络的特点及应用
答：特点：BP神经网络的主要特点是，信号是前向传播的，误差是反向传播的。BP神经网络通常由输入层，隐藏层和输出层组成，各层之间的神经元通过带有权重的连接进行信息传递。
### 6 过拟合和欠拟合的定义及如何解决
答：过拟合是指模型在训练数据上表现很好，但是在验证集测试集上表现较差，这个通常是发生在模型过于复杂，以至于它不仅学习到数据的真实模式，还记住数据中的噪声和偶然性。解决方法：（1）加入早停策略，即在训练过程中，假如训练误差降低，但是验证误差升高，则停止训练。（2）正则化，在误差目标函数中增加一项描述网络复杂度的部分，例如连接权值与阈值的平方和。优化目标加正则化。欠拟合是指模型没有很好地捕捉到数据特征，不能很好地拟合数据。解决方法：（1）添加其他特征项。（2）减少正则化参数。
### 7 维数灾难及解决方案
答：维数灾难是指许多算法在低维空间表现很好，但是输入高维时，算法往往失效，在高维情况下出现的数据样本稀疏，距离计算困难等问题，是所有机器学习方法共同面临的严重障碍。解决方法：（1）降维，即通过某种数学变换，将原始高维属性空间转变为一个低维子空间。
### 8 梯度消失和梯度爆炸的解决方案
答：梯度消失是指在反向传播的过程中，随着梯度逐层传递，梯度逐渐变小，最终导致靠近输入层的梯度接近0，这使得权重更新几乎停止，影响模型的学习能力。解决方法：可以使用适当的激活函数如relu等。可以对权重初始化。可以使用正则化技术，即batch normalization在每一层的输入上应用归一化，使得输入保持相对稳定，有助于梯度传播。梯度爆炸是指在反向传播过程中，随着梯度逐层传递，梯度值不断变大，最终导致梯度值极其大，影响模型的训练稳定性，可能导致权重更新过大。解决方法：梯度裁剪，在反向传播的过程中，将梯度限制在一个合理的范围内。权重初始化，在训练开始时防止梯度包扎，正则化技术，dropout技术也行。学习率的挑战，随着训练的进行，逐步减少学习率。使用Resnet残差连接。
### 9 梯度下降的定义及有关梯度下降算法
梯度下降法是一个一阶最优化算法，通常也称为最速下降法。有关算法如下：（1）批量梯度下降（Batch Gradient Descent, BGD）。（2）随机梯度下降（Stochastic Gradient Descent, SGD）。（3）小批量梯度下降（Mini-Batch Gradient Descent, MBGD）。（4）RMSprop（Root Mean Square Propagation）
### 10 生成式模型和判别式模型有哪些？及其主要区别是什么？
答：生成式模型（Generative Models）具体来说，对于已知变量 X 和未知变量 Y：产生式方法(generative approach)由数据学习到联合概率分布P(X,Y)，然后求出条件概率分布P(Y∣X)作为预测的模型，即产生式模型：P(Y∣X)=P(X,Y)/P(X)。之所以称为产生式模型，是因为模型表示了给定输入 X 产生输出 Y 的产生关系，产生式模型就是产生数据分布的模型。例如：高斯混合模型，朴素贝叶斯分类器，隐马尔可夫模型，生成对抗网络，变分自编码器。判别式模型（Discriminative Models）判别式方法(discriminative approach)由数据直接学到决策函数 f(X) 或条件概率分布 P(Y∣X) 作为预测的模型，即判别式模型。判别式模型关心的是对于给定的输入 X，应该预测什么样的输出 Y，判别模型就是判别数据输出量的模型。例如：逻辑回归，支持向量机，神经网络，k-近邻算法等。主要区别：建模目标，前者是建立联合概率分布P(X,Y)后者是建立条件概率分布P(X|Y)。前者适用于图像生成，数据增强，后者适用于分类，回归等预测任务。前者需要更多的参数，后者需要更少的参数。
### 11 决策树的作用？属性划分方法有哪些？
答：决策树是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。属性划分方法：（1）信息增益：该方法基于信息论中的熵（Entropy）概念。信息增益衡量一个属性在分割数据集时减少熵的程度。选择信息增益最大的属性进行分裂。例如ID3算法。（2）增益率（Gain Ratio）增益率是对信息增益的改进，避免了信息增益偏向多值属性的问题。增益率通过将信息增益除以属性的固有值（Intrinsic Value）来进行调整。（3）基尼指数，基尼指数用于衡量数据集的不纯度或杂乱度。基尼指数越小，数据集的不纯度越低。选择基尼指数最小的属性进行分裂。
### 12 最大似然估计和最大后验估计的异同点
答：极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。极大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。在贝叶斯统计学中，“最大后验概率估计”是后验概率分布的众数。利用最大后验概率估计可以获得对实验数据中无法直接观察到的量的点估计。它与最大似然估计中的经典方法有密切关系，但是它使用了一个增广的优化目标，进一步考虑了被估计量的先验概率分布。所以最大后验概率估计可以看作是规则化（regularization）的最大似然估计。相同点在于：两者都用于模型参数估计。
### 13 最大似然估计和最大后验估计的适用场景
答：最大似然估计（MLE）的适用无先验知识或先验知识不可靠，大样本量。最大后验估计（MAP）的适用存在可靠的先验知识，小样本量，贝叶斯框架。
### 14 在自然语言处理中用到哪些卷积神经网络？
答：（1）MLP；（2）CNN+Dynamic Pooling；（3）CNN+LSTM；（4）将CNN当成RNN来用。
### 15 boosting 的定义和作用
答：Boosting ：一族将弱分类器提升为强分类学习器的算法。Boosting 是一种迭代的算法，每一步都会训练一个新的弱学习器，使其更关注之前模型错误分类的样本。最终的模型是这些弱学习器的加权组合。作用：1.提高准确性：通过组合多个弱学习器，Boosting 可以显著提高模型的准确性。2.减少偏差：通过关注模型难以分类的样本，Boosting 有助于减少模型的偏差，从而提高泛化能力。3.处理不平衡数据：Boosting 在处理类别不平衡的数据时表现良好，因为它通过调整样本权重来提高模型对少数类样本的关注。4.稳健性：Boosting 的迭代过程使得它对噪声和异常值具有一定的鲁棒性
### 16 前馈神经网络的定义和应用范围 
答：前馈神经网络是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。应用：分类、回归、模式识别、功能模拟、时间序列预测
### 17 全连接神经网络的定义
答：连接层实际就是卷积核大小为上层特征大小的卷积运算，卷积后的结果为一个节点，就对应全连接层的一个点。全连接层的坏处就在于其会破坏图像的空间结构，因此人们便开始用卷积层来“代替”全连接层，通常采用1×1的卷积核
### 18 什么叫做泛化能力？如何提高泛化能力？
答：泛化能力（generalization ability）是指机器学习算法对新鲜样本的适应能力。学习的目的是学到隐含在数据对背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。提高方法：重新清洗数据。采用正则化方法。Dropout。提前终止。交叉验证。模型集成。
### 19 什么叫做代价曲线
答：代价曲线（Cost Curve）是一种评估分类模型性能的图形工具，用于比较不同分类器在不同错误代价权重下的表现。与常见的ROC曲线或精度-召回曲线不同，代价曲线特别适合在分类错误的代价（成本）不对称的情况下使用，比如在某些应用中，假阴性和假阳性的代价不同。
### 20 什么叫做深度残差学习？
答：深度残差学习（Deep Residual Learning）是一种深度神经网络结构设计方法，它引入了残差连接（Residual Connection）或跳跃连接（Skip Connection），这条连接跨越了一个或多个层，将输入直接添加到输出中。这种方法最著名的实现是残差网络（ResNet）。优点1. 解决梯度消失和梯度爆炸问题：残差连接提供了直接的梯度传递路径，缓解了梯度消失和梯度爆炸问题，使得训练更深层的网络成为可能。2. 加速收敛：残差连接可以简化优化问题，加速训练过程。3. 提高网络性能：在图像分类、目标检测等任务上，深度残差网络通常表现出色。
### 21 什么叫做朴素贝叶斯分类器
答：朴素贝叶斯分类器（Naive Bayes Classifier）是一类基于贝叶斯定理的监督学习算法，适用于分类任务。其核心思想是通过计算不同类别的概率，选择概率最大的类别作为预测结果。朴素贝叶斯分类器被称为“朴素”的原因在于它假设特征之间是相互独立的，这一假设在大多数实际情况下并不成立，但它在很多应用中仍表现出色。优点：简单高效，处理高维数据。缺点：假设不成立即独立性假设。
### 22 什么叫做 K 近邻学习？
答：分类任务步骤：1. 计算待分类样本与训练集中每个样本的距离。2. 找出距离最近的K个邻居。3. 进行投票（多数表决）：这些邻居所属的类别中出现频率最高的类别作为待分类样本的预测类别。回归任务步骤：1. 计算待预测样本与训练集中每个样本的距离。2. 找出距离最近的K个邻居。3. 取这K个邻居的目标值（输出）的平均值或加权平均值作为待预测样本的预测值。优点在于：简单容易实现，无需训练，实用性强。缺点在于：计算复杂度高，内存开销大，维度灾难。
### 23 什么叫做 K-均值算法？
答：步骤：1. 创建k个点作为k个簇的起始质心（经常随机选择）。2. 分别计算剩下的元素到k个簇中心的相异度（距离），将这些元素分别划归到相异度最低的簇。3. 根据聚类结果，重新计算k个簇各自的中心，计算方法是取簇中所有元素各自维度的算术平均值。4. 将Dataset中全部元素按照新的中心重新聚类。5. 重复第4步，直到聚类结果不再变化。优点在于简单容易实现，计算效率高，可解释性强。缺点在于k值需要事前预定号，对初始值不敏感，容易受到噪声影响。
### 24 为什么对于小的图像数据集一般要采用预训练网络？
答：小的数据集通常不足以支持从零开始训练一个复杂的深层神经网络。深层网络通常需要大量的训练数据来学习和泛化特征，而小数据集可能导致模型过拟合，即在训练数据上表现很好，但在新数据上表现较差。预训练网络通常在大规模数据集（如ImageNet）上训练过，这些数据集包含了大量的多样化图像。通过这种预训练，网络已经学习到了一些通用的低级和中级特征（如边缘、纹理、形状等），这些特征对很多不同的图像识别任务都是有用的。
### 25 评估模型通常用训练集和测试集，为何要加上验证集？
答：通过在验证集上做评估，可以帮助选择最佳的模型和调整模型的超参数（如学习率、正则化参数等）。另外，在训练过程中，如果模型在验证集上的性能不再提升，可以提前停止训练，避免过拟合。
### 26 对于小数据集一般采用什么网络？
答：复杂模型通常会有更多的参数，而小数据集不足以支持这些参数的学习。使用较小且简单的模型可以减少过拟合的风险。可以用浅层神经网络，即只有一层或少数几层的神经网络。线性模型：线性回归、逻辑回归等。
### 27 什么叫做 LSTM 网络？其优点是什么？
答：LSTM（Long Short-Term Memory）网络是一种循环神经网络（RNN）的变体，用于处理和预测时间序列数据，具有记忆长期依赖性的能力。优点：长期依赖性，避免梯度消失和梯度爆炸，可学习的门控机制，灵活性和适用性，可解释性。
### 28 l_0,l_1,l_2范数的计算 
答：l_0非零元素的数目。l_1针对向量，求向量每个值的绝对值之和。l_2向量袁术的平方和的平方根。
### 29 常用概率分布 3.9
答：Bernoulli分布（0-1分布）。高斯分布。指数分布和 Laplace 分布。
### 30 随机梯度下降 5.9
答：机器学习算法中的代价函数通常可以分解为每个样本的代价函数的综合，例如训练数据的负条件对数似然。计算代价函数和参数之间的梯度即可。
### 31 自适应学习算法 8.5
答：
### 32 循环神经网络 10.2
答：RNNs 专门设计用于处理和预测序列数据。序列数据是指有序的数据点，比如时间序列、文本序列等。与传统的前馈神经网络不同，RNN 具有内部的循环结构，使得网络可以记住以前的信息并将其应用于当前的输出。具体来说，RNN 在每个时间步上都接收当前输入和上一个时间步的隐状态（hidden state），并计算当前的隐状态和输出。
### 33 简述 sigmoid 激活函数
答：Sigmoid函数的表达式为S(x)=1/(1+e^-x)，优点在于：1.通过sigmoid函数计算的函数值在0~1之间，使得它适合于处理需要概率输出的任务，如二分类问题。2.Sigmoid函数能够引入非线性特性，使得神经网络可以学习复杂的模式和特征。3.具有光滑的导数，适合用于反向传播算法进行权重更新。缺点在于：1.如果神经网络的层数很多，且每一层的激励函数都采用sigmoid函数，就会产生梯度弥散的问题。因为利用BP函数更新参数的时候，会乘以它的导数，其导数为，这个数值不会超过0.25，所以参数值会一直减小。2.在输入值比较大或者比较小的时候，Sigmoid函数的梯度会趋近于0，导致反向传播时梯度消失。
### 34 为什么更深的网络更好。
答：1. 深层网络可以逐层抽象和组合特征。每一层卷积操作提取和组合前一层的特征，使得后续层能够捕捉到更高级、更抽象的特征。2. 更深的网络意味着更多的参数和更复杂的模型结构。虽然这增加了计算复杂度，但也使得模型可以拟合更复杂的函数，从而更好地捕捉数据中的复杂模式和细节。3. 浅层网络可能难以捕捉复杂的数据模式，容易出现欠拟合问题。更深的网络能够拟合更多复杂的模式，从而减少欠拟合的风险。
### 35 为什么不同的深度学习领域都可以使用 CNN，CNN 解决了这些领域的哪些共性问题？它是如何解决的？
答：它们的共性问题是特征学习的问题，包括提取有意义的局部特征、保持输入数据的结构信息、减少模型参数以防止过拟合、提高计算效率，以及增强模型对输入数据变换（如平移、缩放等）的鲁棒性。CNN通过局部连接和共享权重机制有效地解决了这些问题。局部连接意味着卷积核只在局部区域内扫描输入数据，从而能够捕捉局部特征，这对于图像中的边缘检测或文本中的n-gram提取非常有效。共享权重则大幅减少了模型参数数量，因为同一卷积核在所有输入数据上重复应用，从而提高了计算效率并防止过拟合。此外，CNN的卷积操作能够保持输入数据的空间结构，逐层提取和构建更高级的特征，即从低级特征（如边缘）到高级特征（如物体形状）。池化层进一步通过降采样操作减少特征图的尺寸，同时增强了模型对平移和局部变化的鲁棒性。
### 36 输入数据形状是[10,3,224,224]，卷积核kh = kw = 3, 输出通道数为 64，步幅stride = 1，填充ph = pw = 1。则完成这样一个卷积，一共需要做多少次乘法和加法操作？
答： 3×3卷积核，每个通道的卷积计算需要 9次乘法和8次加法。步骤1：确定输出特征图的尺寸：（224+1*2-3）/1+1=224，即输出是224*224的大小。输出特征图中的元素数： 224×224×64=3211264个，总乘法次数： 3211264×9=28901376 次，总加法次数： 3211264×8=25690112 次
### 37 神经网络的深度和宽度分别指的是什么？
答：深度一般是指隐藏层和输出层的总数。宽度是指每一层中的神经元（节点）的数量。

